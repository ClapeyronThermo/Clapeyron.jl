\documentclass[a4paper,11pt]{article}

% -------------------- Fonts & basic typesetting (pdfLaTeX) --------------------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{newtxtext}
\usepackage{newtxmath}

% -------------------- Math & layout --------------------
% NOTE: do not load amssymb together with newtxmath (it conflicts, e.g. \Bbbk).
\usepackage{mathtools}
\usepackage{geometry}
\geometry{margin=25mm}

% -------------------- Hyperlinks and paths --------------------
\usepackage{hyperref}
\newcommand{\filepath}[1]{\path{#1}}

% -------------------- Bibliography (biblatex + biber) --------------------
\usepackage[
  backend=biber,
  natbib=true,
  style=numeric,
  sorting=nyt
]{biblatex}
\addbibresource{refs.bib}

\title{Clapeyron Tp Flash Benchmark and U-score Evaluation Rules}
\author{Anqing Wang}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
  This document defines a Tp flash benchmark suite (14 instances) and a standardized evaluation protocol for comparing metaheuristic solvers in Clapeyron.
  Following the ``fixed-budget + fixed-target'' philosophy commonly used in CEC-style single-objective competitions \citep{cec2025-github}, we define a per-instance target threshold (\(\mathrm{TGT}\)), a dimension-scaled evaluation budget (\(\mathrm{FE}_{\max}\)), and an aggregation method based on U-scores.
  In addition, we record success rates and interpretable failure tags to support engineering diagnostics.
\end{abstract}

% =============================================================================
\section{Benchmark Definition}\label{sec:benchmark}

\subsection{Goal and scope}\label{sec:benchmark-goal}
The goal is to provide a reusable, reproducible, and implementation-aligned benchmark for Tp flash problems in Clapeyron,
so that changes to the metaheuristic backend (including removing the \texttt{BlackBoxOptim.jl} dependency) can be evaluated fairly and transparently.

\subsection{Problem set provenance and references}\label{sec:benchmark-provenance}
The benchmark instances are derived from the testset ``Tp flash algorithms'' in \filepath{test/test_methods_api_flash.jl}.
The extracted suite is defined in \filepath{test/tpflash/tp_flash_benchmark_cases.jl}.

To avoid any runtime dependence on an ``online'' reference solver, each case stores a reference triplet \((y_i^*, x_i^*, \beta_i^*)\) as numeric literals.
In the Julia case definitions, these are stored under the convention \(g^{+}/x^{+}/\beta^{+}\) (where the code uses \(g\) for the scalar Gibbs objective;
in this document we denote that scalar objective by \(y\)).

\subsection{Notation}\label{sec:benchmark-notation}
For instance \(i\in\{1,\dots,P\}\), algorithm \(a\in\{1,\dots,m\}\), and run \(r\in\{1,\dots,n\}\):
\begin{itemize}
  \item Objective value: \(y\) (implementation name: \(g\), a scalar Gibbs objective).
  \item Reference objective: \(y_i^*\).
  \item Number of species: \(N_{\mathrm{s},i}\), number of phases: \(N_{\mathrm{p},i}\) (\texttt{numphases}).
  \item Dimension:
        \[
          D_i = N_{\mathrm{s},i}\,(N_{\mathrm{p},i}-1).
        \]
  \item Evaluation budget:
        \[
          \mathrm{FE}_{\max,i} = c\,D_i,
        \]
        where \(c\) is a scaling constant (default in the benchmark implementation: \(c=10000\)).
  \item Target threshold: \(\mathrm{TGT}_i\) (Section~\ref{sec:protocol-tgt}).
\end{itemize}

\subsection{Decision variables and bounds}\label{sec:benchmark-decision-variables}
Each Tp flash instance is treated as a bound-constrained scalar optimization problem over
\[
  u \in \mathbb{R}^{D_i}.
\]
The scalar objective is evaluated via Clapeyron's DETPFlash objective implementation \texttt{Obj\_de\_tp\_flash}.
Bounds depend on the per-case \texttt{logspace} flag:
\begin{itemize}
  \item Default (\texttt{logspace=false}): \(u_j \in [0,1]\).
  \item Log-space (\texttt{logspace=true}): \(u_j \in [\ln(4\,\mathrm{eps}(\mathtt{Float64})),\,0]\).
\end{itemize}

% =============================================================================
\section{Evaluation Protocol}\label{sec:protocol}

\subsection{Target threshold}\label{sec:protocol-tgt}
For each instance \(i\), we define a tolerance around the reference objective value \(y_i^*\):
\[
  \mathrm{tol}_i = \max\!\left(100\,\mathrm{eps}(y_i^*),\ \mathrm{rtol}\cdot|y_i^*|+\mathrm{atol}\right),
\]
where \(\mathrm{atol}\) and \(\mathrm{rtol}\) are benchmark parameters (default: \(\mathrm{atol}=\mathrm{rtol}=10^{-14}\)).
The fixed target threshold is then
\[
  \mathrm{TGT}_i \coloneq y_i^* + \mathrm{tol}_i.
\]

\subsubsection{Target gate}\label{sec:protocol-accuracy-target}
A run is considered to have met the \emph{target gate} if it achieves
\begin{equation}
  y \le \mathrm{TGT}_i.
  \label{eq:target-gate}
\end{equation}

\subsection{Trial record mapping}\label{sec:protocol-trial-record}
For each \((a,r,i)\), the solver produces a best-so-far trajectory \(y^{\mathrm{best}}_{a,r,i}(k)\) indexed by the number of objective evaluations \(k\).
We define the gap-to-reference (CEC-style error)
\[
  \mathrm{EV}_{a,r,i}(k) \coloneq y^{\mathrm{best}}_{a,r,i}(k) - y_i^*.
\]
Since \(\mathrm{TGT}_i = y_i^*+\mathrm{tol}_i\), the target gate in Eq.~\eqref{eq:target-gate} is equivalently
\[
  y^{\mathrm{best}}_{a,r,i}(k) \le \mathrm{TGT}_i
  \quad\Longleftrightarrow\quad
  \mathrm{EV}_{a,r,i}(k) \le \mathrm{tol}_i.
\]
The implementation uses the left-hand form \(y\le \mathrm{TGT}_i\) for target checking.

The first evaluation count at which the target is met is recorded as
\[
  \mathrm{FE}^{\mathrm{tgt}}_{a,r,i} = \min\left\{ k \,\middle|\, y^{\mathrm{best}}_{a,r,i}(k) \le \mathrm{TGT}_i \right\},
\]
and is set to \texttt{nothing} if the target is never met.

\subsection{Success criteria and failure tags}\label{sec:protocol-success}
Because Clapeyron is a thermodynamic property library, a run is only counted as a \emph{success} if it produces a physically meaningful solution and passes all gates:
\begin{enumerate}
  \item validity gate (Section~\ref{sec:protocol-validity}),
  \item target gate (Section~\ref{sec:protocol-accuracy-target}),
  \item composition error gate (Section~\ref{sec:protocol-accuracy-x}),
  \item phase-fraction error gate (Section~\ref{sec:protocol-accuracy-beta}).
\end{enumerate}
Otherwise the run is a failure.

For diagnostics, we attach one or more failure tags (multiple tags may apply to the same run):
\begin{itemize}
  \item \texttt{NUMERIC\_FAIL}: NaN/Inf/exception or non-finite objective.
  \item \texttt{CONSTRAINT\_FAIL}: basic physical constraints violated (normalization / non-negativity).
  \item \texttt{TARGET\_FAIL}: did not meet \(y \le \mathrm{TGT}_i\).
  \item \texttt{X\_FAIL}: composition mismatch \(e_x>0.01\).
  \item \texttt{BETA\_FAIL}: phase-fraction mismatch \(e_\beta>0.01\).
\end{itemize}

\subsubsection{Validity gate}\label{sec:protocol-validity}
A run fails the validity gate if any of the following holds:
\begin{enumerate}
  \item The objective value is not finite (NaN/Inf) or evaluation crashed.
  \item Basic constraints are violated (with a small numerical slack):
        \begin{itemize}
          \item For each \emph{active} phase \(k\): \(x_{k,j}\ge -\tau_x\) and \(\sum_j x_{k,j}=1\pm\tau_x\).
          \item Phase fractions: \(\beta_k\ge -\tau_\beta\) and \(\sum_k \beta_k=1\pm\tau_\beta\).
        \end{itemize}
  \item With a fixed \texttt{numphases}, ``vanishing phases'' are allowed:
        any phase with \(\beta_k\le \beta_{\min}\) is treated as inactive (and excluded from the \(x\) checks and the error metrics below).
\end{enumerate}
Default parameters (matching the current benchmark implementation): \(\tau_x=\tau_\beta=10^{-8}\), \(\beta_{\min}=10^{-12}\).

\subsubsection{Composition error gate}\label{sec:protocol-accuracy-x}
We first perform phase matching (a permutation \(\pi\) of phase indices), and compute errors only over active phases
\(\mathcal{A}=\{k \mid \max(\beta_k,\beta^*_{\pi(k)})>\beta_{\min}\}\).
The composition error is defined as
\[
  e_x = \max_{k\in\mathcal{A}}\left\|x_k-x^*_{\pi(k)}\right\|_2,
\]
and the gate requires \(e_x\le 0.01\).

\subsubsection{Phase-fraction error gate}\label{sec:protocol-accuracy-beta}
Similarly, the phase-fraction error is
\[
  e_\beta = \max_{k\in\mathcal{A}} \left|\beta_k-\beta^*_{\pi(k)}\right|,
\]
and the gate requires \(e_\beta\le 0.01\).

The phase-matching permutation \(\pi\) is chosen by lexicographic minimization of \((e_\beta,e_x)\)
(first minimize \(e_\beta\); if tied, minimize \(e_x\)).
Implementation note: the current benchmark enumerates permutations and supports up to \(N_{\mathrm{p}}\le 3\), which covers the present suite.

\subsection{Ranking and U-score}\label{sec:protocol-uscore}

\subsubsection{Final gap used for ranking}\label{sec:protocol-trial-ranking-key}
Let \(\mathrm{FE}^{\mathrm{final}}_{a,r,i}\) be the actual number of evaluations performed by a run (implementation field: \texttt{nfes}).
Typically \(\mathrm{FE}^{\mathrm{final}}_{a,r,i}\le \mathrm{FE}_{\max,i}\) because runs may terminate early due to a wall-clock limit (\texttt{time\_limit})
or stagnation-based early stopping (\texttt{stagnation\_evals}/\texttt{stagnation\_tol}).

For each run we record the final target residual
\[
  \Delta^{\mathrm{final}}_{a,r,i} \coloneq y^{\mathrm{best}}_{a,r,i}(\mathrm{FE}^{\mathrm{final}}_{a,r,i}) - \mathrm{TGT}_i.
\]
\(\Delta^{\mathrm{final}}<0\) indicates the target gate is passed; \(\Delta^{\mathrm{final}}>0\) indicates the run fell short of \(\mathrm{TGT}_i\).

\subsubsection{Per-instance ranking}\label{sec:protocol-ranking}
For a fixed instance \(i\), all runs from all algorithms are pooled, giving \(N=mn\) runs.
They are ranked by:
\begin{enumerate}
  \item Any success outranks any failure.
  \item Success vs.\ success: smaller \(\Delta^{\mathrm{final}}\) is better; ties are broken by smaller \(\mathrm{FE}^{\mathrm{tgt}}\).
  \item Failure vs.\ failure: smaller \(\Delta^{\mathrm{final}}\) is better.
\end{enumerate}
If keys are exactly equal, average ranks are used. The implementation uses ``larger rank is better'' (worst run near 1, best run near \(N\)).

\subsubsection{U-score per instance}\label{sec:protocol-uscore-per-problem}
For instance \(i\) and algorithm \(a\), let the rank-sum be
\[
  \mathrm{SR}_{a,i} = \sum_{r=1}^{n} \mathrm{rank}_{a,r,i}.
\]
Define the correction term \(\mathrm{cf}=\tfrac{n(n+1)}{2}\), and the U-score
\[
  U_{a,i} = \mathrm{SR}_{a,i} - \mathrm{cf}.
\]
The normalized U-score is
\[
  U^{\mathrm{norm}}_{a,i}=\frac{U_{a,i}}{n(N-n)}\in[0,1],\qquad N=mn.
\]
If only one algorithm is present (\(m=1\)), the denominator is zero and \(U^{\mathrm{norm}}\) is undefined (the implementation produces \texttt{NaN});
at least two algorithms are required for U-score comparison.

\subsubsection{Aggregation across instances}\label{sec:protocol-aggregation}
The benchmark aggregates performance across instances using the \emph{mean normalized U-score}:
\[
  \bar U_a^{\mathrm{norm}}=\frac{1}{P}\sum_{i=1}^{P} U^{\mathrm{norm}}_{a,i}.
\]
This matches the current stage-2 implementation, which reports \(\bar U^{\mathrm{norm}}\) (denoted \texttt{mean\_u\_norm}) as the primary score for ranking.
For engineering interpretation, the stage-2 summary also reports the overall success rate.

\printbibliography[title={References}]

\end{document}
